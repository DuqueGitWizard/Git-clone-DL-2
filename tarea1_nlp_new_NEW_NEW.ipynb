{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuqueGitWizard/Git-clone-DL-2/blob/main/tarea1_nlp_new_NEW_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Classificaton Fine-tuning\n",
        "\n",
        "La siguiente tarea consiste en entrenar un modelo de HuggingFace (HF) para realizar la _task_ de _classification_. El dataset para entrenar dicho modelo est谩 predefinido. Sin embargo, el modelo, el tokenizador y el trainer pueden ser totalmente personalizados. Es decir, que tendr茅is que realizar un trabajo de investigaci贸n, de prueba y error, para poder ir aprendiendo y ganando destreza con HF.\n",
        "\n",
        "Recomendaciones:\n",
        "- Durante este proceso, tendr茅is muchas dudas y encontrar茅is muchos errores. Tratad de resolverlas primero por vuestra cuenta, enteniendo la causa del error. Despu茅s con recursos online. Y, finalmente, siempre est谩 el foro, que puede ser utilizado de forma participativa.\n",
        "- No dejeis la tarea para el 煤ltimo d铆a. Los modelos tardan en entrenar. Los problemas no se resuelven en la primera iteraci贸n.\n",
        "\n",
        "Finalmente, se pide:\n",
        "- Limpieza rigurosa en la presentaci贸n del notebook.\n",
        "- El notebook se entrega con todas las celdas ejecutadas.\n",
        "- Los comentarios (opcionales), mejor sobre el c贸digo con '#'.\n",
        "\n",
        "nimo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7w2L4aZ93Y"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "A continuaci贸n, descargar谩s un DatasectDict llamado _glue_. La target es la columna llamada _label_."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "torch.cuda.empty_cache()  # Limpia la memoria cach茅 de CUD\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4RPJkCMFcYvf",
        "outputId": "34fbde11-da4b-460b-87ee-5c67190817d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import os\\nos.environ[\\'CUDA_LAUNCH_BLOCKING\\'] = \"1\"\\nimport torch\\ntorch.cuda.empty_cache()  # Limpia la memoria cach茅 de CUD\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV_9b7GB8Sre",
        "outputId": "d5a77ac9-b8a8-4cdd-b7c3-4f47054ad9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (3.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (1.2.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh) (9.4.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh) (2024.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch accelerate scikit-learn nltk bokeh gensim spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf0dMWJHasU3",
        "outputId": "7577c5e9-cf33-48ed-91e4-e88d02dc2b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "9c12ac8b-c619-48e4-c52a-f29f9343f56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 392702\n",
              "    })\n",
              "    validation_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9815\n",
              "    })\n",
              "    validation_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9832\n",
              "    })\n",
              "    test_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9796\n",
              "    })\n",
              "    test_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9847\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"glue\", \"mnli\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x291zAn4Z93b",
        "outputId": "4b02e04d-21a7-4878-bbcf-c5d0429f05e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['premise', 'hypothesis', 'label', 'idx']\n"
          ]
        }
      ],
      "source": [
        "print(ds['train'].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpRY9AxlZ93c"
      },
      "source": [
        "Lo primero que tendr谩s que hacer es construir un DatasetDict nuevo, llamado **ds_tarea**, que filtre el anterior DatasetDict para:\n",
        "- quedarse con los registros que tengan el contenido de la columna _context_ con menos (estrictamente) de 20 caracteres.\n",
        "- que solo tenga los Datasets de _train_ y _validation_matched_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HrpprwIrIz",
        "outputId": "74d1f2e9-b35e-45b7-fe18-9e3cdcbec20a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "     num_rows: 13635\n",
              " }),\n",
              " 'validation_matched': Dataset({\n",
              "     features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "     num_rows: 413\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ds_tarea = None\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# nos quedamos con los registros que tengan el contenido de la columna context con menos (estrictamente) de 20 caracteres.\n",
        "# nos quedamos solo con los Datasets de train y validation_matched\n",
        "ds_tarea = {\n",
        "    'train': ds['train'].filter(lambda x: len(x['premise']) < 20),\n",
        "    'validation_matched': ds['validation_matched'].filter(lambda x: len(x['premise']) < 20)\n",
        "}\n",
        "### END SOLUTION\n",
        "\n",
        "ds_tarea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ENsg0PlPZ93d"
      },
      "outputs": [],
      "source": [
        "# Celda de control\n",
        "\n",
        "assert len(ds_tarea['train']) == 13635\n",
        "assert len(ds_tarea['validation_matched']) == 413\n",
        "assert set(ds_tarea.keys()) == {'train', 'validation_matched'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQq0hNpZ93d"
      },
      "source": [
        "## EDA\n",
        "\n",
        "Si ten茅is que realizar alguna exploraci贸n del datos, utilizad esta secci贸n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wf_OqTAZ93e",
        "outputId": "dcdf36a5-fbbf-4fcc-f1fb-ad7ec49366ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Vemos las etiquetas 煤nicas en la parte del train del dataset\n",
        "unique_labels = set(ds_tarea['train']['label'])\n",
        "num_labels = len(unique_labels)\n",
        "num_labels # Sacamos el n煤mero de labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPLSrzKyitGi",
        "outputId": "f394a17c-65c2-4d01-bced-9323213f9405"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Vemos las etiquetas 煤nicas en la parte del train del dataset\n",
        "unique_labels = set(ds_tarea['validation_matched']['label'])\n",
        "num_labels = len(unique_labels)\n",
        "num_labels # Sacamos el n煤mero de labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVQ0nkzAZ93e"
      },
      "source": [
        "## Model y Tokenizer\n",
        "\n",
        "Se pide guardar el modelo y el tokenizador en las variables _model_ y _tokenizer_.\n",
        "Aunque no se utilicen hasta m谩s adelante, declaradlos en esta secci贸n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViKrFARKZ93e",
        "outputId": "85db93bb-b5ed-4ae8-fcf9-3ed8dca7f482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, ElectraForSequenceClassification, ElectraTokenizer, AlbertForSequenceClassification, AlbertTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer, XLNetForSequenceClassification, XLNetTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification\n",
        "\"\"\"Para guardar el modelo y el tokenizador, es necesario elegir un modelo pre-entrenado que se adapte a la clasificaci贸n de texto.\n",
        "Usaremos el modelo bert-base-uncased de la libreria de HuggingFace Transformers\"\"\"\n",
        "# Cargamos un modelo (AutoModelForSequenceClassification) para clasificar las secuencias y con from_pretrained(\"bert-base-uncased\") cargamos un modelo preentrenado\n",
        "# he usado bert-base-uncased pero podr铆a haber usado otros como \"roberta-base\", \"distilbert-base-uncased\" o cualquiera de los que he probado m谩s abajo que han obtenido un buen valor de accuracy\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "# AutoTokenizer carga un  tokenizador correspondiente. Con from_pretrained(\"bert-base-uncased\") cargamos el tokenizador correspondiente al modelo preentrenado.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "\"\"\"A continuaci贸n muestro los diferentes modelos de preentrenamiento que se han probado. Las pruebas iniciales se han realizado con un num_train_epochs=1\"\"\"\n",
        "\"\"\"\n",
        "# Accuracy = 0.81 (num_train_epochs=1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" OTRA OPCIN A PROBAR\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Accuracy = 0.799031 (num_train_epochs=1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Accuracy = 0.34 (num_train_epochs=1)\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" OTRA OPCIN A PROBAR\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=3)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Accuracy = 0.34 (num_train_epochs=1)\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=3)\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Accuracy = 0.711864 (num_train_epochs=1)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Accuracy = 0.460048 (num_train_epochs=1)\n",
        "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=3)\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Accuracy = 0.772397 (IMPORTANTE: tarda solo 5 min) (num_train_epochs=1)\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels=3)\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Accuracy = 0.736077 (num_train_epochs=1)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\"\"\"\n",
        "\n",
        "# Por otro lado, con la funci贸n token_funct tokenizamos la columna 'premise'.\n",
        "def token_funct(dataset):\n",
        "    # He observamos que es necesario incluir dentro de la tokenizaci贸n tanto la columna 'premise' como la columna 'hypothesis', ya que con esto le damos al modelo el contexto m谩s correcto para entender la relaci贸n entre\n",
        "    # ambos textos. En caso de incluir 煤nicamente la columna 'premise' obserbamos que el valor del Accuracy no superaba el 0.35\n",
        "    # Por lo tanto, para que el modelo se entrene correctamente utilizando todos los datos que son clave, es necesario incluir la informaci贸n de ambas columnas.\n",
        "    return tokenizer(dataset[\"premise\"],dataset[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "# Aplicamos la funci贸n token_funct a los subconjuntos 'train' y 'validation_matched' del dataset\n",
        "ds_tarea = {\n",
        "    'train': ds_tarea['train'].map(token_funct, batched=True),\n",
        "    'validation_matched': ds_tarea['validation_matched'].map(token_funct, batched=True) # Con 'batched=True' indicamos que la funci贸n no se aplique uno\n",
        "                                                                                        # por uno, sino por lotes (asi mejoramos la eficiencia)\n",
        "}\n",
        "\n",
        "#ds_tarea = ds_tarea.map(token_funct, batched=True)\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaxXeL_iZ93f"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Si ten茅is que realizar alguna modificaci贸n de los datos (no siempre es necesaria, pero algunos modelos preentrenados lo piden), pod茅is utilizar esta secci贸n.\n",
        "\n",
        "Al finalizar la secci贸n, bien si modificais el DatasectDict, bien si no lo modific谩is, lo guardar茅is en __ds_tarea_featured__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PjbDgyWjZ93f"
      },
      "outputs": [],
      "source": [
        "# Celdas de libre uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klgZ1xKDZ93f",
        "outputId": "a1d47b4d-c528-4d4e-930e-96c9b29e9ea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "     num_rows: 13635\n",
              " }),\n",
              " 'validation_matched': Dataset({\n",
              "     features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "     num_rows: 413\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "ds_tarea_featured = ds_tarea # Esta l铆nea tiene sentido en caso de que no se modifique el dataset\n",
        "#ds_tarea_featured['train'] = ds_tarea_featured['train'].rename_column('label','labels')\n",
        "#ds_tarea_featured['validation_matched'] = ds_tarea_featured['validation_matched'].rename_column('label','labels')\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "ds_tarea_featured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ9ibrHJC7gu",
        "outputId": "2cbe79af-a7c5-482f-df3f-d9598cb7d966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['premise', 'hypothesis', 'label', 'idx']\n"
          ]
        }
      ],
      "source": [
        "print(ds['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dctHbVbNZ93f"
      },
      "outputs": [],
      "source": [
        "# Celda de control\n",
        "\n",
        "assert len(ds_tarea_featured['train']) == 13635\n",
        "assert len(ds_tarea_featured['validation_matched']) == 413"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wFpTeIZ93g"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Para poder evaluar el modelo a lo largo del proceso y no esperar a tener toda la ejecuci贸n completa (que podr铆a durar horas), se propone la creaci贸n de una m茅trica que muestre por pantalla la evoluci贸n del entrenamiento.\n",
        "\n",
        "Esta m茅trica se declara en una funci贸n, llamada en este caso _compute_metrics_ y se le pide a los argumentos y al trainer que calculen la m茅trica al final de cada _epoch_ con el _evaluation_dataset_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HwWkVR7wZ93g"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXjU0psZ93g"
      },
      "source": [
        "A continuaci贸n, de forma libre se pide entrenar un modelo de HuggingFace deseado. Se pide usar un Trainer de HuggingFace que tenga los siguientes argumentos como m铆nimo (puede haber m谩s argumentos en todas las variables):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1Z7_7XuVr56O"
      },
      "outputs": [],
      "source": [
        "#pip install accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "unl9NjM1wBCl"
      },
      "outputs": [],
      "source": [
        "#pip install accelerate==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import accelerate\n",
        "\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "args = TrainingArguments(\n",
        "    output_dir='./finetuned1', #\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=5e-5, # Tasa de aprendizaje\n",
        "    logging_dir='./logs', # Guardar logs en este directorio\n",
        "    #gradient_accumulation_steps=2,\n",
        "    #per_device_train_batch_size=8,\n",
        "    #per_device_eval_batch_size=16,\n",
        "    logging_steps=10, # Cada 10  pasos de entrenamiento se va a registrar las m茅tricas.\n",
        "    num_train_epochs=10, # Defnimos 10 茅pocas de entrenamiento del modelo, es decir, se expone el modelo  a todos los datos de entrenamiento durante 10 veces.\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tarea_featured[\"train\"],\n",
        "    eval_dataset=ds_tarea_featured[\"validation_matched\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "A continuaci贸n se entrena el modelo. Se pide no modificar esta celda, solo ejecutar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "uNx5pyRlIrJh",
        "outputId": "815462fc-f798-4690-b0a2-8b01109ff937"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2718' max='17050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2718/17050 17:31 < 1:32:27, 2.58 it/s, Epoch 1.59/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.628900</td>\n",
              "      <td>0.553570</td>\n",
              "      <td>0.808717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-fcf706769f54>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\", trust_remote_code=True)\n"
          ]
        }
      ],
      "source": [
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "from time import time\n",
        "\n",
        "start = time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end = time()\n",
        "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-n80ekFZ93h"
      },
      "outputs": [],
      "source": [
        "# Esta celda tiene que estar ejecutada en la entrega\n",
        "# Se espera un eval_accuracy superior a 0.75\n",
        "# A mayor accuracy no hay mayor nota, con superar el umbral de 0.75 es suficiente\n",
        "\n",
        "results = trainer.evaluate()\n",
        "final_eval_accuracy = results.get(\"eval_accuracy\")\n",
        "\n",
        "print(f\"Final Eval Accuracy: {final_eval_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnTSu13wZ93h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}